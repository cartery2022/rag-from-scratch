{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa888bd9-5ad3-40b4-aa88-375d46c13046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:10.371927Z",
     "start_time": "2026-01-28T03:58:10.320805Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6098b8bf-354d-4eea-ba25-25fe12ba6b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:10.485025Z",
     "start_time": "2026-01-28T03:58:10.393623Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '<your_api_key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d966f427-1a9f-4bc8-b1fa-5df6078b1df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:10.612142Z",
     "start_time": "2026-01-28T03:58:10.544324Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = '<your_api_key>'\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03342a964cc88bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:10.656349Z",
     "start_time": "2026-01-28T03:58:10.613485Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"FALSE\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82a4ee05d86fca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:10.729416Z",
     "start_time": "2026-01-28T03:58:10.676995Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"USER_AGENT\"] = \"rag-from-scratch/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf368e7-ebf6-4469-bfa7-62466184afbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:32.134889Z",
     "start_time": "2026-01-28T03:58:10.731549Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fengwenlong/rag-from-scratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431c9506-c6c0-463b-af77-9291a63f1d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:37.959536Z",
     "start_time": "2026-01-28T03:58:32.268387Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=False  # 重要\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5614c1-121c-4ad5-8609-cc0e4a633ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:40.243142Z",
     "start_time": "2026-01-28T03:58:37.979437Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import MultiVectorRetriever\n",
    "from langchain_core.stores import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f111ca83-3e56-4785-bac3-99948cd8df1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:40.721265Z",
     "start_time": "2026-01-28T03:58:40.258416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='43ff662d-b282-4d7f-8383-c733e6cec9c5', metadata={'doc_id': 'b1c39a61-fd6a-4f8a-9856-680216bdb045'}, page_content='This document is a comprehensive overview of LLM (Large Language Model) powered autonomous agents. It describes the key components of such agents: **Planning, Memory, and Tool Use.**\\n\\n*   **Planning:** Covers task decomposition techniques like Chain of Thought (CoT) and Tree of Thoughts, as well as self-reflection methods like ReAct, Reflexion, Chain of Hindsight (CoH), and Algorithm Distillation (AD) for iterative improvement.\\n*   **Memory:** Discusses different types of memory (sensory, short-term, long-term) and how they relate to LLM agents. It also details Maximum Inner Product Search (MIPS) algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN for efficient information retrieval from external memory stores.\\n*   **Tool Use:** Explores how LLMs can leverage external APIs and tools to extend their capabilities, referencing architectures like MRKL and systems like HuggingGPT and API-Bank.\\n\\nThe document then presents **case studies** of LLM agents in action, including:\\n\\n*   **Scientific Discovery Agents:** ChemCrow and other agents designed for tasks in chemistry, drug discovery, and scientific experimentation.\\n*   **Generative Agents Simulation:** A simulation of 25 virtual characters interacting in a sandbox environment.\\n\\nFinally, the document outlines **challenges** in building LLM-powered agents, including:\\n\\n*   Finite context length\\n*   Difficulties in long-term planning and task decomposition\\n*   Reliability issues with natural language interfaces.\\n\\nIn essence, the document provides a detailed look at the architecture, capabilities, and limitations of LLM-powered autonomous agents, supported by examples and references to relevant research.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729074f9-8bde-4c76-a7da-4cc0e50ed52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T03:58:41.164666Z",
     "start_time": "2026-01-28T03:58:40.724615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three:\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79676de8-c93c-415a-bd1a-ba64065bae27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:09:21.693547Z",
     "start_time": "2026-01-28T04:09:21.654059Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -U ragatouille 下面需要换成老版本的langchain，因为ragatouille最新版本目前还不兼容新版本langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96deaaa9-5101-48a5-a93d-b8af0122430f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:15:21.554234Z",
     "start_time": "2026-01-28T04:15:21.163713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/q9lxz7y96jjbhb93px19fh9c0000gp/T/ipykernel_94823/276443619.py:1: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.retrievers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m      2\u001b[39m RAG = RAGPretrainedModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mcolbert-ir/colbertv2.0\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag-from-scratch/.venv/lib/python3.11/site-packages/ragatouille/__init__.py:21\u001b[39m\n\u001b[32m     14\u001b[39m warnings.warn(\n\u001b[32m     15\u001b[39m     _FUTURE_MIGRATION_WARNING_MESSAGE,\n\u001b[32m     16\u001b[39m     \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m     17\u001b[39m     stacklevel=\u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Ensures the warning points to the user's import line\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.0.9post2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mRAGPretrainedModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mRAGTrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGTrainer\n\u001b[32m     24\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mRAGPretrainedModel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRAGTrainer\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag-from-scratch/.venv/lib/python3.11/site-packages/ragatouille/RAGPretrainedModel.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Literal, Optional, Tuple, TypeVar, Union\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_compressors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDocumentCompressor\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseRetriever\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CorpusProcessor\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.retrievers'"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9bfc1-5f2b-4b9e-9934-9844e3b60646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2317cc1-7406-4115-84c2-d0527a4ad22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929e4fd-2175-465d-bd88-664f67caa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cbbc7-bd6e-488d-9419-740a62eb097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}