{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "971b8eb2-54ca-4b16-b046-079d526b406e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:48.388142Z",
     "start_time": "2026-01-28T06:43:48.310406Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66c5fd1-8da9-416f-b854-1b70d207d606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:48.453402Z",
     "start_time": "2026-01-28T06:43:48.389951Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '<your_api_key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb22812-822d-4320-be3c-ab0c52356914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:48.470655Z",
     "start_time": "2026-01-28T06:43:48.456291Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = '<your_api_key>'\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81abf78a27e5082e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:40:31.072123Z",
     "start_time": "2026-01-28T07:40:30.992766Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['COHERE_API_KEY'] = '<your_api_key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c05d51253f2e9b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:48.485657Z",
     "start_time": "2026-01-28T06:43:48.472318Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"FALSE\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e2cd104db798bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:48.496813Z",
     "start_time": "2026-01-28T06:43:48.486952Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"USER_AGENT\"] = \"rag-from-scratch/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f201eec1-6ed0-4236-9594-286894574779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:53.662503Z",
     "start_time": "2026-01-28T06:43:48.497692Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "# from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    # embedding=CohereEmbeddings()\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b733a230-d217-4ee0-8482-ab4380e7be4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:53.726811Z",
     "start_time": "2026-01-28T06:43:53.712187Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:53.813561Z",
     "start_time": "2026-01-28T06:43:53.728906Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=False  # 重要\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: [s for s in x.split(\"\\n\") if s.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:43:57.537436Z",
     "start_time": "2026-01-28T06:43:53.814956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here are 4 search queries related to \"What is task decomposition for LLM agents?\":', '1.  \"Task decomposition in LLM agents explained\"', '2.  \"LLM agents task decomposition strategies\"', '3.  \"Benefits of task decomposition for large language models\"', '4.  \"Examples of task decomposition for LLM agents\"']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = pickle.dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (pickle.loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T06:44:00.520683Z",
     "start_time": "2026-01-28T06:43:57.562304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large, complex tasks into smaller, more manageable subgoals. This allows the agent to handle complex tasks more efficiently. Task decomposition can be achieved through methods like Chain of Thought (CoT), Tree of Thoughts, simple prompting, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:42:08.744045Z",
     "start_time": "2026-01-28T07:42:07.232317Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Re-rank\n",
    "compressor = CohereRerank(\n",
    "    model=\"rerank-english-v3.0\",\n",
    "    top_n=3,\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}