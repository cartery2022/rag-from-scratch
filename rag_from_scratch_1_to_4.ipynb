{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Rag From Scratch: Overview\n",
    "\n",
    "These notebooks walk through the process of building RAG app(s) from scratch.\n",
    "\n",
    "They will build towards a broader understanding of the RAG langscape, as shown here:\n",
    "\n",
    "![Screenshot 2024-03-25 at 8.30.33 PM.png](attachment:c566957c-a8ef-41a9-9b78-e089d35cf0b7.png)\n",
    "\n",
    "## Environment\n",
    "\n",
    "`(1) Packages`"
   ],
   "id": "8d028fe17c2f0d19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain",
   "id": "27f171c3c79798a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`(2) LangSmith`\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ],
   "id": "c5b2d310bedfddb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''"
   ],
   "id": "6583f5a78cccd0cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`(3) API Keys`",
   "id": "acb3be30c287b9cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.environ['GOOGLE_API_KEY'] = ''\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ],
   "id": "88d75a6e03546232"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"FALSE\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ],
   "id": "e67187c2f66b1389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ[\"USER_AGENT\"] = \"rag-from-scratch/1.0\"",
   "id": "8a5c10c17089cb22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`(3) API Keys`",
   "id": "9fd41ea18cea648f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ['OPENAI_API_KEY'] = <your-api-key>",
   "id": "2b3596707fb45e0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Overview\n",
    "\n",
    "[RAG quickstart](https://python.langchain.com/docs/use_cases/question_answering/quickstart)"
   ],
   "id": "5004709db123055c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_classic.hub import pull\n",
    "import bs4\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "client_settings = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=True,               # 可选\n",
    "    persist_directory=\"./chroma_db\",   # 可选\n",
    ")\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding = embedding,\n",
    "                                    client_settings=client_settings,\n",
    "                                    collection_name=\"rag_demo\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = pull(\"rlm/rag-prompt\")          # 可能是 str\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=False  # 重要\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "id": "98fa5577bbcdcfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2: Indexing\n",
    "\n",
    "![Screenshot 2024-02-12 at 1.36.56 PM.png](attachment:d1c0f19e-1f5f-4fc6-a860-16337c1910fa.png)"
   ],
   "id": "200ca680240fd058"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ],
   "id": "3b2b7fa7b80936fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[Count tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considering [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)",
   "id": "d538171e60c7a361"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ],
   "id": "9792913d5e3e589e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)",
   "id": "5b51d199268e2d40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query_result = embedding.embed_query(question)\n",
    "document_result = embedding.embed_query(document)"
   ],
   "id": "b87557824a3c45b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is recommended (1 indicates identical) for OpenAI embeddings.",
   "id": "273b9bc6947307cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ],
   "id": "440ecd2f0b2e348b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)",
   "id": "539de96ec9080829"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ],
   "id": "7fd9858d2eb4ce21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
    "\n",
    "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ],
   "id": "ccb5980a823fc15f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ],
   "id": "9f8e478b2c5ce2db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)",
   "id": "8fbc73140f2f3d29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3: Retrieval",
   "id": "8f5fc0a4882ea0fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Index\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ],
   "id": "19386bb5ee948fb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docs = retriever.invoke(\"What is Task Decomposition?\")",
   "id": "abadfae3f2ac1068"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(docs)",
   "id": "4c67f33eeb833679"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 4: Generation\n",
    "\n",
    "![Screenshot 2024-02-12 at 1.37.38 PM.png](attachment:f9b0e284-58e4-4d33-9594-2dad351c569a.png)"
   ],
   "id": "1ec2d46d64f452ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ],
   "id": "40053687416469d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ],
   "id": "76e0959711f0a1e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ],
   "id": "e34a86dbed95af6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)",
   "id": "d47eeeb15011ead2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "rag_chain.invoke(\"What is Task Decomposition?\")",
   "id": "83e716840a993f38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "15698a847e4cfd57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
